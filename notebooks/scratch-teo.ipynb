{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from groq import Groq\n",
    "from src.utils import get_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning secret from environment variable `GROQ_API_KEYS`=`gs...a9`\n"
     ]
    }
   ],
   "source": [
    "api_keys = get_secret(\"GROQ_API_KEYS\").split(';')\n",
    "client = Groq(api_key=api_keys[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====BEGIN-PROMPT===\n",
      "Please translate the following single choice question and the 4 answer options regarding regulations of Los Andes University to English:\n",
      "PREGUNTA: ¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?\n",
      "\n",
      "Opción A. Tener promedio acumulado superior a 4.5\n",
      "Opción B. Estar en el 3% superior del promedio histórico\n",
      "Opción C. Tener promedio acumulado de 5.0\n",
      "Opción D. Promedio superior a 4.0\n",
      "\n",
      "Your answer should consist of only 5 lines.\n",
      "The expected format of those lines is:\n",
      "QUESTION: <translation of PREGUNTA to *English*>\n",
      "OPTION A: <translation of Opción A. to *English*>\n",
      "OPTION B: <translation of Opción B. to *English*>\n",
      "OPTION C: <translation of Opción C. to *English*>\n",
      "OPTION D: <translation of Opción D. to *English*>\n",
      "\n",
      "=====END-PROMPT======\n",
      "\n",
      "====BEGIN-COMPLETION===\n",
      "QUESTION: Nivel de promedio acumulado necesario para Cum Laude\n",
      "Opción A: Nivel de promedio acumulado superior a 4.5\n",
      "Opción B: Nivel de promedio histórico superior al 3%\n",
      "Opción C: Nivel de promedio acumulado de 5.0\n",
      "Opción D: Nivel de promedio acumulado superior a 4.0\n",
      "=======END-COMPLETION=======\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TMPL = \"\"\"Please translate the following single choice question and the 4 answer options regarding regulations of Los Andes University to English:\n",
    "PREGUNTA: {question}\n",
    "\n",
    "{options}\n",
    "\n",
    "Your answer should consist of only 5 lines.\n",
    "The expected format of those lines is:\n",
    "QUESTION: <translation of PREGUNTA to *English*>\n",
    "OPTION A: <translation of Opción A. to *English*>\n",
    "OPTION B: <translation of Opción B. to *English*>\n",
    "OPTION C: <translation of Opción C. to *English*>\n",
    "OPTION D: <translation of Opción D. to *English*>\n",
    "\"\"\"\n",
    "\n",
    "GROQ_MODEL = \"llama-3.2-1b-preview\"\n",
    "\n",
    "# rec = \"¿Qué requisito de grado puede variar según el programa específico de maestría?,El promedio acumulado mínimo,El trabajo de grado,La asistencia a clases,El requisito de inglés\".split(\",\")\n",
    "rec=\"¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?,Tener promedio acumulado superior a 4.5,Estar en el 3% superior del promedio histórico,Tener promedio acumulado de 5.0,Promedio superior a 4.0\"\n",
    "\n",
    "rec_parts = rec.split(\",\")\n",
    "question = rec_parts[0]\n",
    "options = [f\"Opción {let}. {ans}\" for let, ans in zip(\"ABCD\", rec_parts[1:5])]\n",
    "\n",
    "prompt = PROMPT_TMPL.format(question=question, options=\"\\n\".join(options))\n",
    "\n",
    "print(f\"====BEGIN-PROMPT===\\n{prompt}\\n=====END-PROMPT======\\n\")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "                        messages=[{\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }],\n",
    "                        model=GROQ_MODEL,\n",
    "                        temperature=0.0\n",
    "                    )\n",
    "\n",
    "completion = chat_completion.choices[0].message.content\n",
    "print(f\"====BEGIN-COMPLETION===\\n{completion}\\n=======END-COMPLETION=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?\" \n",
      "\n",
      " translates to \"What is the accumulated average required to receive the Cum Laude degree?\"\n",
      "=======END-COMPLETION=======\n",
      "\n",
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"Tener promedio acumulado superior a 4.5\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"Total acumulado superior a 4.5\"\n",
      "=======END-COMPLETION=======\n",
      "\n",
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"Estar en el 3% superior del promedio histórico\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"Estar en el 3% superior del promedio histórico\" translates to \"Being at the 3% above the historical average\".\n",
      "=======END-COMPLETION=======\n",
      "\n",
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"Tener promedio acumulado de 5.0\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"Total acumulado de 5.0\"\n",
      "=======END-COMPLETION=======\n",
      "\n",
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"Promedio superior a 4.0\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"Superior a 4.0\"\n",
      "=======END-COMPLETION=======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TMPL = \"\"\"Please translate the following to English:\n",
    "\"{sentence}\"\n",
    "Please provide ONLY A SINGLE English translation\"\"\"\n",
    "\n",
    "GROQ_MODEL = \"llama-3.2-1b-preview\"\n",
    "\n",
    "# rec = \"¿Qué requisito de grado puede variar según el programa específico de maestría?,El promedio acumulado mínimo,El trabajo de grado,La asistencia a clases,El requisito de inglés\".split(\",\")\n",
    "rec=\"¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?,Tener promedio acumulado superior a 4.5,Estar en el 3% superior del promedio histórico,Tener promedio acumulado de 5.0,Promedio superior a 4.0\"\n",
    "\n",
    "rec_parts = rec.split(\",\")\n",
    "question = rec_parts[0]\n",
    "options = [f\"Opción {let}. {ans}\" for let, ans in zip(\"ABCD\", rec_parts[1:5])]\n",
    "\n",
    "for a_str in rec_parts:\n",
    "    prompt = PROMPT_TMPL.format(sentence=a_str)\n",
    "\n",
    "    print(f\"====BEGIN-PROMPT===\\n{prompt}\\n=====END-PROMPT======\")\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "                            messages=[{\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": prompt\n",
    "                            }],\n",
    "                            model=GROQ_MODEL,\n",
    "                            temperature=0.0\n",
    "                        )\n",
    "\n",
    "    completion = chat_completion.choices[0].message.content\n",
    "    print(f\"====BEGIN-COMPLETION===\\n{completion}\\n=======END-COMPLETION=======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning secret from environment variable `HF_API_KEY`=`hf...Gk`\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/teo/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpu_mem_used_MB': 188.15,\n",
       " 'gpu_mem_free_MB': 5832.51,\n",
       " 'gpu_mem_total_MB': 6020.66}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import src.utils as ut\n",
    "import src.trainer as trn\n",
    "import src.finetuning as ft\n",
    "\n",
    "from src.utils import LLAMA_MODEL_ID, login_to_hf_hub, load_test_df\n",
    "\n",
    "login_to_hf_hub()\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(LLAMA_MODEL_ID)\n",
    "\n",
    "ut.gpu_mem_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Estatutos-Universidad-de-los-Andes-2020-ratificados-MEN-RQ.translated.txt               :  49,682 bytes,    689 lines\n",
      "../data/reglamento-maestria-web-2024.translated.txt                                             :  54,466 bytes,    686 lines\n",
      "concat_files_to_str: returning 108,502 characters, whole text at: tokenized_txt_dataset.concat.txt\n",
      "TokenizedTxtDs.__init__: len(all_text)=108,502\n",
      "n_toks_raw: 20674 start_idx: 0 end_idx: 18606\n",
      "n_toks_sampled:  18,606  input_ids length (padded):  18,688 block_size: 128 n_blocks: 146\n",
      "../data/Estatutos-Universidad-de-los-Andes-2020-ratificados-MEN-RQ.translated.txt               :  49,682 bytes,    689 lines\n",
      "../data/reglamento-maestria-web-2024.translated.txt                                             :  54,466 bytes,    686 lines\n",
      "concat_files_to_str: returning 108,502 characters, whole text at: tokenized_txt_dataset.concat.txt\n",
      "TokenizedTxtDs.__init__: len(all_text)=108,502\n",
      "n_toks_raw: 20674 start_idx: 18606 end_idx: 20674\n",
      "n_toks_sampled:   2,068  input_ids length (padded):   2,176 block_size: 128 n_blocks: 17\n",
      "len(ds): 292 max_stride_mult: 2\n"
     ]
    }
   ],
   "source": [
    "from src.txt_dataset import TokenizedTxtDataset\n",
    "\n",
    "text_fpaths = [\n",
    "    Path(\"../data/Estatutos-Universidad-de-los-Andes-2020-ratificados-MEN-RQ.translated.txt\"),\n",
    "    Path(\"../data/reglamento-maestria-web-2024.translated.txt\"),\n",
    "]\n",
    "\n",
    "train_ds = TokenizedTxtDataset(text_fpaths,\n",
    "                block_size=128,\n",
    "                stride=64,\n",
    "                tokenizer=TOKENIZER,\n",
    "                start_pct=0.0,\n",
    "                end_pct=90.0\n",
    "            )\n",
    "\n",
    "valid_ds = TokenizedTxtDataset(text_fpaths,\n",
    "                block_size=128,\n",
    "                stride=64,\n",
    "                tokenizer=TOKENIZER,\n",
    "                start_pct=90.0,\n",
    "                end_pct=100.0\n",
    "            )\n",
    "\n",
    "print(\"len(ds):\", len(train_ds), \"max_stride_mult:\", train_ds.max_stride_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(train_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=False)\n",
    "for batch in train_dl:\n",
    "    break\n",
    "\n",
    "## batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpu_mem_used_MB': 2662.79,\n",
       " 'gpu_mem_free_MB': 3357.87,\n",
       " 'gpu_mem_total_MB': 6020.66}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ut.load_raw_model()\n",
    "ut.gpu_mem_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros sin LoRA: 167,772,160 || Parámetros con LoRA: 3,407,872  || Porcentaje de parámetros con LoRA: 1.99%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpu_mem_used_MB': 3212.25,\n",
       " 'gpu_mem_free_MB': 2808.41,\n",
       " 'gpu_mem_total_MB': 6020.66}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.freeze_and_install_lora(model, lora_rank=16)\n",
    "ut.gpu_mem_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "\n",
      "Initializing trainer: device: cuda - len(train_ds)=292, len(valid_ds)=34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = ut.module_device(model)\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "trainer = trn.Trainer(\n",
    "    train_ds=train_ds,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    valid_ds=valid_ds,\n",
    "    valid_batch_size=VALID_BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_next_token_loss(model: nn.Module, batch: dict[str, Tensor]) -> Tensor:\n",
    "    return model(input_ids=batch['input_ids'],\n",
    "                 attention_mask=batch['attention_mask'],\n",
    "                 labels=batch['input_ids']).loss\n",
    "\n",
    "trainer.train(model,\n",
    "              loss_fun=pred_next_token_loss,\n",
    "              max_steps=70,\n",
    "              accum_grad_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando por 70 pasos. 1 epochs\n",
      "Nota Importante:\n",
      "    El `Train Loss` que se reporta se calcula únicamente sobre los datos de los últimos 8 pasos de entrenamiento.\n",
      "    El `Valid Loss` es sobre *todos* los datos de validación\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step    5 - Train loss: 3.049                     (tokens/sec: 1326) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    5 -                    Valid loss: 2.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   10 - Train loss: 2.624                     (tokens/sec: 1424) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   10 -                    Valid loss: 2.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   15 - Train loss: 2.451                     (tokens/sec: 1431) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   15 -                    Valid loss: 2.166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   20 - Train loss: 2.454                     (tokens/sec: 1432) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   20 -                    Valid loss: 2.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   25 - Train loss: 2.372                     (tokens/sec: 1426) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   25 -                    Valid loss: 2.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   30 - Train loss: 2.337                     (tokens/sec: 1424) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   30 -                    Valid loss: 2.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   35 - Train loss: 2.295                     (tokens/sec: 1387) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   35 -                    Valid loss: 2.136\n",
      "train_dl exhausted, resetting... epoch_cnt=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   40 - Train loss: 2.017                     (tokens/sec: 1421) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   40 -                    Valid loss: 2.153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   45 - Train loss: 1.945                     (tokens/sec: 1096) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   45 -                    Valid loss: 2.196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   50 - Train loss: 1.930                     (tokens/sec: 1415) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   50 -                    Valid loss: 2.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   55 - Train loss: 1.884                     (tokens/sec: 1419) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   55 -                    Valid loss: 2.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   60 - Train loss: 1.758                     (tokens/sec: 1411) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   60 -                    Valid loss: 2.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   65 - Train loss: 1.765                     (tokens/sec: 1417) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   65 -                    Valid loss: 2.210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   69 - Train loss: 1.881                     (tokens/sec: 1413) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   69 -                    Valid loss: 2.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step   70 - Train loss: 1.628                     (tokens/sec: 1400) - estimating loss on 'validation' dataset: 100%|██████████| 5/5 [00:00<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   70 -                    Valid loss: 2.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): LoraLinear(\n",
       "            (linear_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (k_proj): LoraLinear(\n",
       "            (linear_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          )\n",
       "          (v_proj): LoraLinear(\n",
       "            (linear_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          )\n",
       "          (o_proj): LoraLinear(\n",
       "            (linear_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.save(model, \"../data/r16-e70.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-maia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
