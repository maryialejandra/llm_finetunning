{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from groq import Groq\n",
    "from src.utils import get_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning secret from environment variable `GROQ_API_KEYS`=`gs...a9`\n"
     ]
    }
   ],
   "source": [
    "api_keys = get_secret(\"GROQ_API_KEYS\").split(';')\n",
    "client = Groq(api_key=api_keys[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====BEGIN-PROMPT===\n",
      "Please translate the following single choice question and the 4 answer options regarding regulations of Los Andes University to English:\n",
      "PREGUNTA: ¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?\n",
      "\n",
      "Opción A. Tener promedio acumulado superior a 4.5\n",
      "Opción B. Estar en el 3% superior del promedio histórico\n",
      "Opción C. Tener promedio acumulado de 5.0\n",
      "Opción D. Promedio superior a 4.0\n",
      "\n",
      "Your answer should consist of only 5 lines.\n",
      "The expected format of those lines is:\n",
      "QUESTION: <translation of PREGUNTA to *English*>\n",
      "OPTION A: <translation of Opción A. to *English*>\n",
      "OPTION B: <translation of Opción B. to *English*>\n",
      "OPTION C: <translation of Opción C. to *English*>\n",
      "OPTION D: <translation of Opción D. to *English*>\n",
      "\n",
      "=====END-PROMPT======\n",
      "\n",
      "====BEGIN-COMPLETION===\n",
      "QUESTION: Nivel de promedio acumulado necesario para Cum Laude\n",
      "Opción A: Nivel de promedio acumulado superior a 4.5\n",
      "Opción B: Nivel de promedio histórico superior al 3%\n",
      "Opción C: Nivel de promedio acumulado de 5.0\n",
      "Opción D: Nivel de promedio acumulado superior a 4.0\n",
      "=======END-COMPLETION=======\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TMPL = \"\"\"Please translate the following single choice question and the 4 answer options regarding regulations of Los Andes University to English:\n",
    "PREGUNTA: {question}\n",
    "\n",
    "{options}\n",
    "\n",
    "Your answer should consist of only 5 lines.\n",
    "The expected format of those lines is:\n",
    "QUESTION: <translation of PREGUNTA to *English*>\n",
    "OPTION A: <translation of Opción A. to *English*>\n",
    "OPTION B: <translation of Opción B. to *English*>\n",
    "OPTION C: <translation of Opción C. to *English*>\n",
    "OPTION D: <translation of Opción D. to *English*>\n",
    "\"\"\"\n",
    "\n",
    "GROQ_MODEL = \"llama-3.2-1b-preview\"\n",
    "\n",
    "# rec = \"¿Qué requisito de grado puede variar según el programa específico de maestría?,El promedio acumulado mínimo,El trabajo de grado,La asistencia a clases,El requisito de inglés\".split(\",\")\n",
    "rec=\"¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?,Tener promedio acumulado superior a 4.5,Estar en el 3% superior del promedio histórico,Tener promedio acumulado de 5.0,Promedio superior a 4.0\"\n",
    "\n",
    "rec_parts = rec.split(\",\")\n",
    "question = rec_parts[0]\n",
    "options = [f\"Opción {let}. {ans}\" for let, ans in zip(\"ABCD\", rec_parts[1:5])]\n",
    "\n",
    "prompt = PROMPT_TMPL.format(question=question, options=\"\\n\".join(options))\n",
    "\n",
    "print(f\"====BEGIN-PROMPT===\\n{prompt}\\n=====END-PROMPT======\\n\")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "                        messages=[{\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }],\n",
    "                        model=GROQ_MODEL,\n",
    "                        temperature=0.0\n",
    "                    )\n",
    "\n",
    "completion = chat_completion.choices[0].message.content\n",
    "print(f\"====BEGIN-COMPLETION===\\n{completion}\\n=======END-COMPLETION=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?\" \n",
      "\n",
      " translates to \"What is the accumulated average required to receive the Cum Laude degree?\"\n",
      "=======END-COMPLETION=======\n",
      "\n",
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"Tener promedio acumulado superior a 4.5\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"Total acumulado superior a 4.5\"\n",
      "=======END-COMPLETION=======\n",
      "\n",
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"Estar en el 3% superior del promedio histórico\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"Estar en el 3% superior del promedio histórico\" translates to \"Being at the 3% above the historical average\".\n",
      "=======END-COMPLETION=======\n",
      "\n",
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"Tener promedio acumulado de 5.0\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"Total acumulado de 5.0\"\n",
      "=======END-COMPLETION=======\n",
      "\n",
      "====BEGIN-PROMPT===\n",
      "Please translate the following to English:\n",
      "\"Promedio superior a 4.0\"\n",
      "Please provide ONLY A SINGLE English translation\n",
      "=====END-PROMPT======\n",
      "====BEGIN-COMPLETION===\n",
      "\"Superior a 4.0\"\n",
      "=======END-COMPLETION=======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TMPL = \"\"\"Please translate the following to English:\n",
    "\"{sentence}\"\n",
    "Please provide ONLY A SINGLE English translation\"\"\"\n",
    "\n",
    "GROQ_MODEL = \"llama-3.2-1b-preview\"\n",
    "\n",
    "# rec = \"¿Qué requisito de grado puede variar según el programa específico de maestría?,El promedio acumulado mínimo,El trabajo de grado,La asistencia a clases,El requisito de inglés\".split(\",\")\n",
    "rec=\"¿Cuál es el nivel de promedio acumulado necesario para recibir el grado Cum Laude?,Tener promedio acumulado superior a 4.5,Estar en el 3% superior del promedio histórico,Tener promedio acumulado de 5.0,Promedio superior a 4.0\"\n",
    "\n",
    "rec_parts = rec.split(\",\")\n",
    "question = rec_parts[0]\n",
    "options = [f\"Opción {let}. {ans}\" for let, ans in zip(\"ABCD\", rec_parts[1:5])]\n",
    "\n",
    "for a_str in rec_parts:\n",
    "    prompt = PROMPT_TMPL.format(sentence=a_str)\n",
    "\n",
    "    print(f\"====BEGIN-PROMPT===\\n{prompt}\\n=====END-PROMPT======\")\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "                            messages=[{\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": prompt\n",
    "                            }],\n",
    "                            model=GROQ_MODEL,\n",
    "                            temperature=0.0\n",
    "                        )\n",
    "\n",
    "    completion = chat_completion.choices[0].message.content\n",
    "    print(f\"====BEGIN-COMPLETION===\\n{completion}\\n=======END-COMPLETION=======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import src.utils as ut\n",
    "import src.trainer as trn\n",
    "import src.finetuning as ft\n",
    "\n",
    "from src.utils import LLAMA_MODEL_ID, login_to_hf_hub, load_test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning secret from environment variable `HF_API_KEY`=`hf...Gk`\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/teo/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login_to_hf_hub()\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(LLAMA_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "line = \"## Article\"\n",
    "re.search(r\"(Article|Art\\.|Artículo) *([0-9]+)\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Estatutos-Universidad-de-los-Andes-2020-ratificados-MEN-RQ.translated.txt               :  49,682 bytes,    689 lines\n",
      "../data/reglamento-maestria-web-2024.translated.txt                                             :  54,466 bytes,    686 lines\n",
      "concat_files_to_str: returning 108,502 characters, whole text at: tokenized_txt_dataset.concat.txt\n",
      "TokenizedTxtDs.__init__: len(all_text)=108,502\n",
      "n_toks_raw: 20674 start_idx: 0 end_idx: 18606\n",
      "n_toks_sampled:  18,606  input_ids length (padded):  18,688 block_size: 256 n_blocks: 73\n",
      "../data/Estatutos-Universidad-de-los-Andes-2020-ratificados-MEN-RQ.translated.txt               :  49,682 bytes,    689 lines\n",
      "../data/reglamento-maestria-web-2024.translated.txt                                             :  54,466 bytes,    686 lines\n",
      "concat_files_to_str: returning 108,502 characters, whole text at: tokenized_txt_dataset.concat.txt\n",
      "TokenizedTxtDs.__init__: len(all_text)=108,502\n",
      "n_toks_raw: 20674 start_idx: 18606 end_idx: 20674\n",
      "n_toks_sampled:   2,068  input_ids length (padded):   2,304 block_size: 256 n_blocks: 9\n",
      "len(ds): 146 max_stride_mult: 2\n"
     ]
    }
   ],
   "source": [
    "from src.txt_dataset import TokenizedTxtDataset\n",
    "\n",
    "text_fpaths = [\n",
    "    Path(\"../data/Estatutos-Universidad-de-los-Andes-2020-ratificados-MEN-RQ.translated.txt\"),\n",
    "    Path(\"../data/reglamento-maestria-web-2024.translated.txt\"),\n",
    "]\n",
    "\n",
    "train_ds = TokenizedTxtDataset(text_fpaths,\n",
    "                block_size=256,\n",
    "                stride=128,\n",
    "                tokenizer=TOKENIZER,\n",
    "                start_pct=0.0,\n",
    "                end_pct=90.0\n",
    "            )\n",
    "\n",
    "\n",
    "valid_ds = TokenizedTxtDataset(text_fpaths,\n",
    "                block_size=256,\n",
    "                stride=128,\n",
    "                tokenizer=TOKENIZER,\n",
    "                start_pct=90.0,\n",
    "                end_pct=100.0\n",
    "            )\n",
    "\n",
    "print(\"len(ds):\", len(train_ds), \"max_stride_mult:\", train_ds.max_stride_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,    271,  23417,  ...,    279,  85597,    323],\n",
       "         [ 53412,  71056,   8812,  ...,    409,   2537,   1628],\n",
       "         [  5201,    315,   3885,  ...,  67613,    409,   2537],\n",
       "         [   288,  49357,     25,  ...,     11,    719,   1070]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=False)\n",
    "for batch in train_dl:\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ut.load_raw_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros sin LoRA: 167,772,160 || Parámetros con LoRA: 3,407,872  || Porcentaje de parámetros con LoRA: 1.99%\n"
     ]
    }
   ],
   "source": [
    "ft.freeze_and_install_lora(model, lora_rank=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "\n",
      "Initializing trainer: device: cuda - len(train_ds)=146, len(valid_ds)=18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = ut.module_device(model)\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "trainer = trn.Trainer(\n",
    "    train_ds=train_ds,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    valid_ds=valid_ds,\n",
    "    valid_batch_size=VALID_BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_next_token_loss(model: nn.Module, batch: dict[str, Tensor]) -> Tensor:\n",
    "    return model(input_ids=batch['input_ids'],\n",
    "                 attention_mask=batch['attention_mask'],\n",
    "                 labels=batch['input_ids']).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-maia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
